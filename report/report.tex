\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage[margin=3cm]{geometry}
\usepackage[hidelinks]{hyperref}

\setlength{\fboxsep}{0.05\textwidth}

\newcommand{\commentEq}[1]{%
  \text{\phantom{(#1)}} \tag{#1}
}
\geometry{margin=1in, headsep=0.25in}

\begin{document}
\setcounter{section}{0}
\thispagestyle{empty}

\noindent\fbox{\begin{minipage}{0.9\textwidth}
  \begin{center}
    \section*{CS 7140: Advanced Machine Learning}
	\subsection*{Project Report:  Weight Uncertainty in Neural Networks}
  \end{center}
  
  \noindent\textbf{Instructor}
  \smallskip
  
  Jan-Willem van de Meent (\url{j.vandemeent@northeastern.edu})\\
  
  \noindent\textbf{Students}
  \smallskip

  Colin Kohler (\url{kohler.c@husky.neu.edu})\\
  Andrea Baisero (\url{baisero.a@husky.neu.edu})
\end{minipage}}
\bigskip

\newcommand\RR{{\mathbb{R}}}

\section{Introduction}
A well known problem with artificial netural networks is their tendency to 
overfit their training data. This overfitting results in a extremely low 
error on the training data and a large error on the test data. More concretely,
we can state that the network has become overly confident about the amount of
uncertainty in the training data and therfor makes predictions on the test
data which are unrealisticly optimistic. There are two main techiniques used
to combat this problem: eary stopping and regularization. 

Early stopping is the most naive method to prevent overfitting and simply 
involves stopping the network's training once the test error has begun to
increase. The second technique, regularization, involves modifying the loss 
function by adding a penalty term. Through the addition of this penalty, the
network is forced to learn a more general model of the data thereby avoiding
overfitting. In this report we will implement and test a regularization 
technique known as \textit{Bayes by Backprop}\cite{} which utilizes 
variational Bayesian learning to introduce uncertainty in the weight of the 
neural network.

\subsection{Artificial Neural Networks}
Before discussing Bayes by Backprop, we must take a slight deviation from
the norm to view neural networks in a more probablistic manner. A neural 
network is typically viewed as a collection of connected units, each with 
their own weight, which takes some input signal and produces an output
signal by passing it through these units. Learning is then acheived by 
using gradient descent to optimize these weights with respect to some loss
function. 

We will formalize this notition by defining a neural network as a probablistic
model, $P(y|x,w)$ where $x \in \mathcal{R}^P, y \in \mathcal{Y}$. Using this 
model, we can view neural networks as a function which assigns a probability
to each possible outcome $y$ given some input $x$ through the use of the set
of weights $w$. These weights are still learned through the use of gradient 
descent however we can now view this learning as the maximum likelihood 
estimate (MLE) of the weights given some data $D=(X,Y)$:
\begin{align*}
  w^{MLE} &= \arg\max_w \log P(D|w) \\ 
  &= \arg\max_w \sum_i \log P(y_i|x_i, w) 
\end{align*}

As Bayes by Backprop is a regularization technique we have to expand upon this
estimate to include regularization by introducing a prior on the weights $w$ 
and then finding the maximum a posteriori (MAP) weights:
\begin{align*}
  w^{MAP} &= \arg\max_w \log P(w|D) \\
  &= \arg\max_w \log P(D|w) + \log P(w) 
\end{align*}
Depending on the distribution of the prior placed on $w$ we can get different
types of regularization. If $P(w)$ is a gaussian, then we will have L2 
regularization, which is also known as weight decay. If $P(w)$ is a
Laplace distribution, then we will have L1 regularization.


\subsection{Black Box Variational Inference}

\section{Bayes by Backprop} \label{bayes_by_backprop}

\section{Experiments}
In order to test the preformance of the Bayes by Backprob algorithm, we
implemented a Bayesian neural network (BNN), both from scratch and with PyTorch,
and trained it on various tasks using Bayes by Backprop. We then tested its
preformance against a plain feedforward neural network with no regularization
and an additional feedforward neural network with dropout layers. All three
of these neural networks were traing using minibatch stocastic gradient descent
on the same data.

The results listed below were generated using our PyTroch implemntations of 
Bayesian neural networks and feedforward neural networks. The code for these
implementations is hosted on Github at \url{https://github.com/ColinKohler/
cs7140_bnn}. As a note to readers, while we were able to create our own 
implementations of these networks using NumPy and Autograd, which can also
be found at the above git repository, they ended up being too slow as they had
to be run on the CPU. Therefor, we recommend using a deep learning library in
order to allow for faster computation using GPUs. 

The PyTorch implementations are fairly straight forward and follow the 
algorithm detailed in section \ref{bayes_by_backprop} with one important 
additional detail. When initializing the parameters for the variance of the 
weights (the $\rho$ parameters) in the BNN, we had to set them to a small
negative number. Our best results came from sampling these parameters from
a normal distribution with a mean of $-3$ and variance of $0.01$. The reason
for this atypically initialization is that while we want to initialize the
weights in our neural network to small values, if we do this for the 
variational parameters in the BNN these variances will be quite large. Thus,
when we sample the weights from these parameters they will be large which can
lead to a unstable network.

\subsection{Classification}
\paragraph{Data --- MNIST}
\paragraph{Results}

\subsection{Regression}
\paragraph{Data --- Synthetic Data}
\paragraph{Results}

\subsection{Contextual Bandits}

Contextual Bandits (CB) is a standard problem domain type from the field of
Reinforcement Learning (RL), and can be interpreted as either an extension of
Multi-Armed Bandits (MAB), or a precursor to full-blown Markov Decision
Processes (MDP).

% The agent receives a i.d.d.\ contexts sampled from an unknown distribution
% $P(x)$.

% In a CB problem, the agent receives a \emph{context} $x\in\RR^d$ sampled from
% an unknown distribution $P(x)$.

% The general CB problem is as follows:  The CB agent receives a \emph{context}
% $x\in\RR^d$;  This process is repeated either indefinitely or for a fixed
% number of time steps (the horizon);  the objective is that of maximizing the
% long-term average of all received rewards.

% At each time step, the CB agent receives a \emph{context} $x\in\RR^d$

% The CB problem consists in the usual

The methods for solving CB problems can be roughly split into three
categories:
%
\begin{itemize}
  %
  \item Non-Bayesian
  %
  \item Bayesian (Optimal)
  %
  \item Bayesian (Heuristic)
  %
\end{itemize}

\paragraph{Data --- Mushroom Domain}

\paragraph{Results}

\section{Conclusions}

\end{document}
