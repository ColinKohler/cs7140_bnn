\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage[margin=3cm]{geometry}
\usepackage[hidelinks]{hyperref}

\setlength{\fboxsep}{0.05\textwidth}

\newcommand{\commentEq}[1]{%
  \text{\phantom{(#1)}} \tag{#1}
}
\geometry{margin=1in, headsep=0.25in}

\begin{document}
\setcounter{section}{0}
\thispagestyle{empty}

\noindent\fbox{\begin{minipage}{0.9\textwidth}
  \begin{center}
    \section*{CS 7140: Advanced Machine Learning}
	\subsection*{Project Report:  Weight Uncertainty in Neural Networks}
  \end{center}
  
  \noindent\textbf{Instructor}
  \smallskip
  
  Jan-Willem van de Meent (\url{j.vandemeent@northeastern.edu})\\
  
  \noindent\textbf{Students}
  \smallskip

  Colin Kohler (\url{kohler.c@husky.neu.edu})\\
  Andrea Baisero (\url{baisero.a@husky.neu.edu})
\end{minipage}}
\bigskip

\newcommand\RR{{\mathbb{R}}}

\section{Introduction}
A well known problem with artificial netural networks is their tendency to 
overfit their training data. This overfitting results in a extremely low 
error on the training data and a large error on the test data. More concretely,
we can state that the network has become overly confident about the amount of
uncertainty in the training data and therfor makes predictions on the test
data which are unrealisticly optimistic. There are two main techiniques used
to combat this problem: eary stopping and regularization. 

Early stopping is the most naive method to prevent overfitting and simply 
involves stopping the network's training once the test error has begun to
increase. The second technique, regularization, involves modifying the loss 
function by adding a penalty term. Through the addition of this penalty, the
network is forced to learn a more general model of the data thereby avoiding
overfitting. In this report we will implement and test a regularization 
technique known as \textit{Bayes by Backprop}\cite{} which utilizes 
variational Bayesian learning to introduce uncertainty in the weight of the 
neural network.

\subsection{Artificial Neural Networks}
Before discussing Bayes by Backprop, we must take a slight deviation from
the norm to view neural networks in a more probablistic manner. A neural 
network is typically viewed as a collection of connected units, each with 
their own weight, which takes some input signal and produces an output
signal by passing it through these units. Learning is then acheived by 
using gradient descent to optimize these weights with respect to some loss
function. 

We will formalize this notition by defining a neural network as a probablistic
model, $P(y|x,w)$ where $x \in \mathcal{R}^P, y \in \mathcal{Y}$. Using this 
model, we can view neural networks as a function which assigns a probability
to each possible outcome $y$ given some input $x$ through the use of the set
of weights $w$. These weights are still learned through the use of gradient 
descent however we can now view this learning as the maximum likelihood 
estimate (MLE) of the weights given some data $D=(X,Y)$:
\begin{align*}
  w^{MLE} &= \arg\max_w \log P(D|w) \\ 
  &= \arg\max_w \sum_i \log P(y_i|x_i, w) 
\end{align*}

As Bayes by Backprop is a regularization technique we have to expand upon this
estimate to include regularization by introducing a prior on the weights $w$ 
and then finding the maximum a posteriori (MAP) weights:
\begin{align*}
  w^{MAP} &= \arg\max_w \log P(w|D) \\
  &= \arg\max_w \log P(D|w) + \log P(w) 
\end{align*}
Depending on the distribution of the prior placed on $w$ we can get different
types of regularization. If $P(w)$ is a gaussian, then we will have L2 
regularization, which is also known as weight decay. If $P(w)$ is a
Laplace distribution, then we will have L1 regularization.


\subsection{Black Box Variational Inference}


\section{Bayes by Backprop}

\section{Experiments}

\subsection{Classification}
\paragraph{Data --- MNIST}
\paragraph{Results}

\subsection{Regression}
\paragraph{Data --- Synthetic Data}
\paragraph{Results}

\subsection{Contextual Bandits}

Contextual Bandits (CB) is a standard problem domain type from the field of
Reinforcement Learning (RL), and can be interpreted as either an extension of
Multi-Armed Bandits (MAB), or a precursor to full-blown Markov Decision
Processes (MDP).

% The agent receives a i.d.d.\ contexts sampled from an unknown distribution
% $P(x)$.

% In a CB problem, the agent receives a \emph{context} $x\in\RR^d$ sampled from
% an unknown distribution $P(x)$.

% The general CB problem is as follows:  The CB agent receives a \emph{context}
% $x\in\RR^d$;  This process is repeated either indefinitely or for a fixed
% number of time steps (the horizon);  the objective is that of maximizing the
% long-term average of all received rewards.

% At each time step, the CB agent receives a \emph{context} $x\in\RR^d$

% The CB problem consists in the usual

The methods for solving CB problems can be roughly split into three
categories:
%
\begin{itemize}
  %
  \item Non-Bayesian
  %
  \item Bayesian (Optimal)
  %
  \item Bayesian (Heuristic)
  %
\end{itemize}

\paragraph{Data --- Mushroom Domain}

\paragraph{Results}

\section{Conclusions}

\end{document}
