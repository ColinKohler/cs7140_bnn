\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage[margin=3cm]{geometry}
\usepackage[hidelinks]{hyperref}

\usepackage{bm}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{todonotes}

\setlength{\fboxsep}{0.05\textwidth}

\newcommand{\commentEq}[1]{%
  \text{\phantom{(#1)}} \tag{#1}
}
\geometry{margin=1in, headsep=0.25in}

\begin{document}
\setcounter{section}{0}
\thispagestyle{empty}

\noindent\fbox{\begin{minipage}{0.9\textwidth}
  \begin{center}
    \section*{CS 7140: Advanced Machine Learning}
	\subsection*{Project Report:  Weight Uncertainty in Neural Networks}
  \end{center}
  
  \noindent\textbf{Instructor}
  \smallskip
  
  Jan-Willem van de Meent (\url{j.vandemeent@northeastern.edu})\\
  
  \noindent\textbf{Students}
  \smallskip

  Colin Kohler (\url{kohler.c@husky.neu.edu})\\
  Andrea Baisero (\url{baisero.a@husky.neu.edu})
\end{minipage}}
\bigskip

\newcommand\Aset{{\mathcal{A}}}
\newcommand\Xset{{\mathcal{X}}}

\newcommand\RR{{\mathbb{R}}}

\newcommand\Ind{{\mathbb{I}}}
\newcommand\Exp{{\mathbb{E}}}

\section{Introduction}
A well known problem with artificial netural networks is their tendency to 
overfit their training data. This overfitting results in a extremely low 
error on the training data and a large error on the test data. More concretely,
we can state that the network has become overly confident about the amount of
uncertainty in the training data and therfor makes predictions on the test
data which are unrealisticly optimistic. There are two main techiniques used
to combat this problem: eary stopping and regularization. 

Early stopping is the most naive method to prevent overfitting and simply 
involves stopping the network's training once the test error has begun to
increase. The second technique, regularization, involves modifying the loss 
function by adding a penalty term. Through the addition of this penalty, the
network is forced to learn a more general model of the data thereby avoiding
overfitting. In this report we will implement and test a regularization 
technique known as \textit{Bayes by Backprop}\cite{} which utilizes 
variational Bayesian learning to introduce uncertainty in the weight of the 
neural network.

\subsection{Artificial Neural Networks}
Before discussing Bayes by Backprop, we must take a slight deviation from
the norm to view neural networks in a more probablistic manner. A neural 
network is typically viewed as a collection of connected units, each with 
their own weight, which takes some input signal and produces an output
signal by passing it through these units. Learning is then acheived by 
using gradient descent to optimize these weights with respect to some loss
function. 

We will formalize this notition by defining a neural network as a probablistic
model, $P(y|x,w)$ where $x \in \mathcal{R}^P, y \in \mathcal{Y}$. Using this 
model, we can view neural networks as a function which assigns a probability
to each possible outcome $y$ given some input $x$ through the use of the set
of weights $w$. These weights are still learned through the use of gradient 
descent however we can now view this learning as the maximum likelihood 
estimate (MLE) of the weights given some data $D=(X,Y)$:
\begin{align*}
  w^{MLE} &= \arg\max_w \log P(D|w) \\ 
  &= \arg\max_w \sum_i \log P(y_i|x_i, w) 
\end{align*}

As Bayes by Backprop is a regularization technique we have to expand upon this
estimate to include regularization by introducing a prior on the weights $w$ 
and then finding the maximum a posteriori (MAP) weights:
\begin{align*}
  w^{MAP} &= \arg\max_w \log P(w|D) \\
  &= \arg\max_w \log P(D|w) + \log P(w) 
\end{align*}
Depending on the distribution of the prior placed on $w$ we can get different
types of regularization. If $P(w)$ is a gaussian, then we will have L2 
regularization, which is also known as weight decay. If $P(w)$ is a
Laplace distribution, then we will have L1 regularization.


\subsection{Black Box Variational Inference}

\section{Bayes by Backprop}\label{sec:bayes_by_backprop}

\section{Experiments}
In order to test the preformance of the Bayes by Backprob algorithm, we
implemented a Bayesian neural network (BNN), both from scratch and with PyTorch,
and trained it on various tasks using Bayes by Backprop. We then tested its
preformance against a plain feedforward neural network with no regularization
and an additional feedforward neural network with dropout layers. All three
of these neural networks were traing using minibatch stocastic gradient descent
on the same data.

The results listed below were generated using our PyTroch implemntations of 
Bayesian neural networks and feedforward neural networks. The code for these
implementations is hosted on Github at \url{https://github.com/ColinKohler/
cs7140_bnn}. As a note to readers, while we were able to create our own 
implementations of these networks using NumPy and Autograd, which can also
be found at the above git repository, they ended up being too slow as they had
to be run on the CPU\@. Therefor, we recommend using a deep learning library in
order to allow for faster computation using GPUs. 

The PyTorch implementations are fairly straight forward and follow the 
algorithm detailed in \cref{sec:bayes_by_backprop} with one important 
additional detail. When initializing the parameters for the variance of the 
weights (the $\rho$ parameters) in the BNN, we had to set them to a small
negative number. Our best results came from sampling these parameters from
a normal distribution with a mean of $-3$ and variance of $0.01$. The reason
for this atypically initialization is that while we want to initialize the
weights in our neural network to small values, if we do this for the 
variational parameters in the BNN these variances will be quite large. Thus,
when we sample the weights from these parameters they will be large which can
lead to a unstable network.

\subsection{Classification --- MNIST}
We trained and tested networks of various sizes on the MNIST digits
dataset\cite{} which consists of $70,000$ images of size 28 by 28 pixels.
As we were comparing our results to the paper which introduced Bayes by 
Backprop, we followed the same preprocessing steps as they did and 
preprocessed the images by divinging the pixel values by $126$. The networks
in question consisted of two hidden layers of rectified linear units (ReLU)
and a log softmax output layer with $10$ outputs, representing the probability
that the given image is that digit. Again in order to compare our results to
that of the original work, we trained on $50,000$ images, tested on
$10,000$ images, and used the last $10,000$ images as a validation set.

While in the originial work the authors ran cross validation to select the best
hyperparameters, we did not have the time or computational resources to run this
costly operation and resorted to hand optimizing the hyperparameters instead.
For our BNN, we ended up using a learning rate of $0.1$, a minibatch size of 
$128$, and averaged over $1$ sample. We used the spike-slab prior detailed in
section \ref{bayes_by_backprop} with $\pi = 0.25$, $\sigma_1 = 0.75$, and 
$\sigma_2 = 0.1$. For the simple feedforward network and the network with 
dropout, we used a learning rate of $0.001$ and a minibatch size of $128$.
The dropout rate was the standard $0.5$. Although we tested networks of various
sizes, the results listed below come from training two-layer networks with
$400$ hidden units in each layer.

\paragraph{Results}

\subsection{Regression}
\paragraph{Data --- Synthetic Data}
\paragraph{Results}

\subsection{Contextual Bandits}

Contextual Bandits (CB) are a standard problem from the field of reinforcement
learning which can be interpreted as either an extension of Multi-Armed Bandits
(with the introduction of contexts/states), or a restriction of fully
observable Markov Decision Processes (without the agent's ability to influence
the context/state generative process).  A CB agent iteratively receives
a \emph{context} $\bm x\in\Xset$, and is subsequently prompted to select an
\emph{action} from a predefined set $\bm a\in\Aset$.  The agent then receives
a real \emph{reward} $\bm r\in\RR$ sampled from an unknown stationary
distribution $\bm r\sim P(\bm r\mid \bm x, \bm a)$, before receiving a new
independent context and repeating the process for a given (potentially
indetermined) amount of time steps.  The goal of the agent is that of
maximizing the long term average of all the received rewards $\Exp\left[\bm
r\right]$.  

Agents typically start out with very limited prior knowledge concerning the
process dynamics (the context or reward distributions).  As a consequence, good
strategies for the maximization of the long-term expected rewards necessarily
involve the (locally sub-optimal) sub-goal of gathering more information about
the process itself.  This notion is know as the \emph{exploration/exploitation
trade-off}, and is a fundamental principle which will determine the success of
a CB agent.  Viewing CB agents from the perspective of how they implement the
exploration/exploitation trade-off, we can roughly split them into three
categories:
%
\begin{description}
  %
  \item[Non-Bayesian ---]  This type of approach learns a point-estimate of the
    expected return associated with the context-action pair $(\bm x, \bm a)$.
    These methods learn to model the information necessary for exploitative
    purposes, but inherently lack the means to implement an explorative
    strategy.  As a consequence, they are paired with an independent
    explorative strategy (typically $\epsilon$-greedy) which occasionally
    selects random actions.
  %
  \item[Bayesian (Optimal) ---]  This type of approach learns a model of the
    full conditional distribution $P(\bm r\mid \bm x, \bm a)$, and uses it to
    implement long-term optimal strategies which plan multiple steps ahead.
    While this approach typically obtains the best performance, it is also the
    hardest to define and implement.
  %
  \item[Bayesian (Heuristic) ---]  This type of approach also requires
    a conditional model over rewards, but implement simpler action-selection
    strategies which, albeit still dependent on the model's learned uncertainty,
    provide fewer guarantees compared to Basyesian-optimal strategies.  

    If the model uncertainties are appropriately learned, this type of
    approach typically performs better than non-Bayesian ones due to the fact that
    the explration rate becomes dependent on the model uncertainty (rather than
    constant) and that the exploration targets the more promising actions (rather
    than being uniform across all of them).
  %
\end{description}

To demonstrate the fact that the uncertainty learned by Bayes-by-Backprop is
appropriate for a pluritude of tasks, the authors propose to use Thompson
sampling---one of the simplest yet most effective Bayesian non-optimal
approaches---to showcase that the resulting agent's performance is better than
other standard non-Bayesian approaches.  In Thompson sampling, each action is
selected with the same likelihood that it is the optimal action under the
current reward model\footnote{Note that, while Thompson sampling is a very
sensible approach, it is \emph{not} a Basyesian optimal strategy due to the
fact that exploration is a byproduct of the model uncertainty, rather than an
explicit consideration made to maximize long-term returns.}.  

\paragraph{Data --- Mushroom Domain} The Mushroom Data Set\footnote{Mushroom
Data Set: \url{https://archive.ics.uci.edu/ml/datasets/mushroom}.} contains
8124 instances corresponding to 23 species of North American mushrooms
described in terms of 23 discrete attributes, one of which indicates whether
the mushroom is toxic or edible, and the other 22 of which are related to their
shape, size, color, population, habitat, etc.

The context is created by concatenating the one-hot encoding of the 22
attributes, resulting in a context $\bm x\in \{0, 1\}{}^{117}$.  The binary
variable $\bm y\in\{0, 1\}$ is used to denote whether the mushroom is edible,
whereas the action set $\Aset = \{0, 1\}$ indicates whether the agent will eat
the mushroom.  The rewards are produced as follows
(Note that, while it is possible to obtain a better reward by eating a toxic
mushroom due to luck, in average it is better to avoid it.)

\begin{center}
\begin{tabular}{ccl}
  $\bm y$ & $\bm a$ & $\bm r$ \\ 
  \toprule
  --- & avoid & $0$ (Deterministic) \\
  edible & eat & $5$ (Deterministic) \\
  toxic & eat & Categorical\@($\{5, -35\}$; $p=\{0.5, 0.5\}$) \\  
\end{tabular}
\end{center}

In this experimental setup, the authors train a NN on the regression problem of
predicting the expected reward corresponding to a context-action pair $(\bm x,
\bm a)$.  
Because a BNN trained via Bayes-by-Backprop provides
uncertainty measures, the proposed model is paired with Thompson sampling,
while the baselines consist of standard NNs, trained via Backpropagation, and
paired with greedy, $.05$-greedy and $.1$-greedy action-selection strategies.


\paragraph{Results} 

The performance of a CB agent can be measured in terms of how much
\emph{regret} it accumulates over time.  The regret corresponding to a single
interaction is defined as the difference between the obtained reward, and the
reward which would have been obtained by an oracle who knows the mushroom's
toxicity.  In this domain, the oracle would always eat an edible mushroom
(receiving reward $5$), and always avoid a toxic one (receiving reward $0$).
The resulting regret function is:
%
\begin{equation}
  %
  \operatorname{Regret}(\bm r, \bm y) = \bm r - 5 \, \Ind\left[ \bm
  y = 1 \right]
  %
\end{equation}

\Cref{fig:cb_cumregret} depicts the cumulative regrets obtained by the
non-Bayesian CB agents and the one trained with Bayes-by-Backprop.  

While we were able (with a caveat, explained soon) to implement the
non-Bayesian agents and confirm similar results as those reported in
\cref{fig:cb_cumregret}, we were unfortunately unable to do the same for the
Bayesian agent trained via Bayes-by-Backprop.

\todo[inline]{Finish caveat and description}


\begin{figure}
  %
  \centering\includegraphics[width=.5\textwidth]{figures/cb_cumregret.png}
  %
  \caption{Cumulative regret obtained by non-Bayesian CB agents, and the BNN
  agent trained via Bayes-by-Backprop.  Figure taken directly
  from~\cite{blarg}.}\label{fig:cb_cumregret}
  %
\end{figure}


\section{Conclusions}

\end{document}
